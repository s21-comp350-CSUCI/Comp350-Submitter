# How the back end works _v 1.1_
Change log found [here](.backendchangelog.md)
### Before the backend runs, a student submission is generated...  
When a student submission is generated, two objects will bu uploaded to the
submitter S3 bucket. One is a single .py file provided by the student(s),
and a .json file generated by the front end logic containing all the requisite data for the backend to begin processing 
the submission.  
This json file will contain the following:
* submissionid  : a 128 bit MD5 Hash unique to the specific students, admin, event,  and problem.
* admin         : The admin's unique identifier (e.g "m.soltys").  
* event         : A string identifier that is unique to the admin.
* problem       : A string identifier unique to the event.
* tokens        : A(n) list/array of tokens generated by the front end upon event registration.  

An example:  
[submission_example.json](./json/submission_example.json)
```
{
  "subid": "fc55c0190dde2bc413d8d1e79fb8cca2",
  "admin": "m.soltys",
  "event": "aws_labs",
  "problem": "lab5_containers",
  "tokens": [
    "d823640ab3b0f7a4a2bc9fc89661e940",
    "240669d4326dea48bba75e066b90b76f",
    "4b31d568d86a9350d746c7c2fe9bf5c8"
  ]
}        
```  
The file structure of the S3 bucket is very important. We will be using a pseudo file system. As of this writing the top
level directory of the bucket will contain two sub directories:  
* /submissions/ 
* /output/  
* we may have to consider placing all the event parameters into the bucket as well...  

When a new submission is submitted, both a json and python file will be generated in a new directory. For example:  
`/submissions/m.soltys/aws_labs/lab5_containers/fc55c0190dde2bc413d8d1e79fb8cca2/fc55c0190dde2bc413d8d1e79fb8cca2.json`   
and  
`/submissions/m.soltys/aws_labs/lab5_containers/fc55c0190dde2bc413d8d1e79fb8cca2/fc55c0190dde2bc413d8d1e79fb8cca2.py`  

## A new object/file in the S3 bucket triggers a lambda function...
  
A new object in the submitter S3 bucket triggers a lambda function written in python. This lambda function will analyse 
the name of the new object, and if the new object is a student submission json file, a message is generated and sent 
to the submitter SQS queue. This message is a json file and, as of this writing, contains just one key : value pair :
```
{
  "subdata" : "/submissions/m.soltys/aws_labs/lab5_containers/fc55c0190dde2bc413d8d1e79fb8cca2/fc55c0190dde2bc413d8d1e79fb8cca2.json"
}        
``` 
## The back end makes its inglorious entrance...  
### The following is an overview of how the script to process student submissions
The back end's [processStudentsTestCases.py](./processStudentTestCases.py) will first retrieve the next message from the SQS queue, using our own 
back end module of functions documented [here](./.backendmethods.md) we begin.  
### Retrieve neccessary files and data  
Our script begins by  
* extracting the name of the json file from the SQS message.  
* creating a directory using the subid from the subdata filename, for example  
  * `/home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/`   
* changing present working directories into the newly cerated directory, from there retrieving both the newly submitted 
  json and python files from the S3 bucket, for example:  
  * `/home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/fc55c0190dde2bc413d8d1e79fb8cca2.json`  
  * `/home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/fc55c0190dde2bc413d8d1e79fb8cca2.py`
    
### Query database for test case parameters
From the data in the json file, our script will then   
* query the database for the test inputs and outputs, and generate files for comparison, for example `test.in` and `test.out`.   

These files will be formatted so that individual test case input for the specific problem is placed on a new line in `test.in`, 
and the appropriate output expected for each input is placed on a new line in `test.out`.   
The directory structure at this point:  
* `/home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/fc55c0190dde2bc413d8d1e79fb8cca2.json`  
* `/home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/fc55c0190dde2bc413d8d1e79fb8cca2.py`  
* `/home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/test.in`  
* `/home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/test.out`  

### Execute submitted code in Python 3.9 container  
At this point we have the code to execute, the inputs to run, and the appropriate outputs for comparison. We are using a
python 3 docker image with a custom, executable script [executeSubmission.py](./executeSubmiaaion.py) installed that will execute the submitted 
code against all provided inputs from test.in, generating and generating the output file. Our script will now:
* Create a new thread that will in turn execute the appropriate `docker run` command.  
  * `docker run -it --rm --name="subidfc55c0190dde2bc413d8d1e79fb8cca2" -v "$PWD":/usr/src/submitter -w /usr/src/submitter 
    python:3.9 executeSubmission.py fc55c0190dde2bc413d8d1e79fb8cca2`  
    
This will spin up our container and keep it alive until it finishes executing. The present working directory gets bind-mounted
to a newly created directory in the container `/usr/src/submitter`. Read more on the container and how it works [here](./docker/Docker.md).
Since the default for bind mount is read-write, this allows our script in the container to generate an output that can be read 
by our main thread. It is important that our script's  
* Main thread __MUST__ wait for the worker thread to finish executing before moving on.  


### Return output to the front-end via submitter S3 bucket and clean house  

_At the time of this writing it is understood that the way to return the output generated by the user's submission is for the backend to
upload the newly generated `fc55c0190dde2bc413d8d1e79fb8cca2.out` to the submitter S3 bucket for the front end to retrieve._   
  
Now that our worker thread has finished executing, our script's current directory should contain:  
* `/home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/fc55c0190dde2bc413d8d1e79fb8cca2.json`
* `/home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/fc55c0190dde2bc413d8d1e79fb8cca2.py`
* `/home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/test.in`
* `/home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/test.out`
* `/home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/fc55c0190dde2bc413d8d1e79fb8cca2.out`

Our script will now:  
* upload `fc55c0190dde2bc413d8d1e79fb8cca2.out` to submitter S3 bucket.  
* change working directories up one to `/home/ec2-user/`.  
* delete the directory '/home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/'

Finally, now that the submission has been processed, the working space has been cleaned, and the output generated has been
sent to the next destination, our script must:
* delete the message that was originally retrieved from the submitter SQS queue.  

It is important to remember that when our script initially retrieved the message from the SQS queue, that it doers not _remove_ the 
message from the queue, it only hides it from any other process being able to retrieve it for the ammount of time defined in 
the parameters for the SQS instance.  
